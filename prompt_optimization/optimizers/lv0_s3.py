"""Enhanced optimizer for lv0_s3 discipline verification with DSPy 2024-2025 signature metadata support.

This optimizer demonstrates modern prompt optimization with automatic signature metadata
extraction for verification tasks, enabling declarative DSPy programming and enhanced reasoning.

Features:
- âœ… GEPA optimization with signature metadata extraction  
- âœ… DSPy ChainOfThought predictor for enhanced reasoning and verification
- âœ… Multi-field output with reasoning for comprehensive metric evaluation
- âœ… Automatic compliance with DSPy 2024-2025 best practices
- âœ… Enhanced prompt artifacts with declarative programming support

Environment Variables:
    GEPA_GEN_MODEL: Generation model (default: gpt-4o-mini)
    GEPA_REFLECTION_MODEL: Reflection model (default: same as GEPA_GEN_MODEL)
"""

import logging
import sys
from typing import Any, Dict, List

import dspy
from dotenv import load_dotenv
from prompt_optimization.optimizers.common import (
    load_json_training,
    run_optimization,
)

logger = logging.getLogger(__name__)

load_dotenv()


TRAINING_DATA_PATH = "data/prompts_training_data/lv0_s3.json"


class AcademicDisciplineVerification(dspy.Signature):
    """You are an expert at verifying academic disciplines.
    Determine if the given term represents a legitimate academic discipline or field of study.
    Consider whether it's taught at universities, has research communities, or represents a recognized field of knowledge.
    """

    term = dspy.InputField(desc="Term to verify as academic discipline")
    is_discipline = dspy.OutputField(
        desc="Boolean indicating if term is an academic discipline"
    )
    reasoning = dspy.OutputField(desc="Explanation for the decision")


class DisciplineVerifier(dspy.Module):
    """DSPy module for verifying academic disciplines."""

    def __init__(self):
        super().__init__()
        # Use ChainOfThought for better reasoning and optimization
        self.prog = dspy.ChainOfThought(AcademicDisciplineVerification)

    def forward(self, term: str) -> dspy.Prediction:
        """Verify if a term is an academic discipline."""
        return self.prog(term=term)


def load_training_data(
    file_path: str = TRAINING_DATA_PATH,
) -> List[Dict[str, Any]]:
    """Load training data from JSON file."""
    return load_json_training(file_path)


def create_training_data() -> tuple[List[str], List[Dict[str, Any]]]:
    """Create training data from JSON file."""
    training_data = load_training_data()

    inputs = []
    outputs = []

    for item in training_data:
        inputs.append(item["keyword"])

        output = {
            "is_discipline": item["expected"],
            "reasoning": f"Based on evidence from: {', '.join(item.get('evidence_colleges', []))}",
        }
        outputs.append(output)

    return inputs, outputs


def prepare_dspy_examples(
    inputs: List[str], outputs: List[Dict[str, Any]]
) -> List[dspy.Example]:
    """Convert training data to DSPy examples."""
    examples = []

    for term, expected_output in zip(inputs, outputs):
        example = dspy.Example(
            term=term,
            is_discipline=str(expected_output["is_discipline"]).lower(),
            reasoning=expected_output["reasoning"],
        ).with_inputs("term")

        examples.append(example)

    return examples


def metric_with_feedback(gold, pred, trace=None, pred_name=None, pred_trace=None):
    """
    Metric function for GEPA that evaluates discipline verification accuracy.
    Returns dspy.Prediction with score and feedback.
    """
    try:
        # Harden boolean conversion with None guarding
        pred_is_discipline_raw = getattr(pred, 'is_discipline', None)
        gold_is_discipline_raw = getattr(gold, 'is_discipline', None)
        
        # Convert to string and handle None cases
        pred_is_discipline_str = str(pred_is_discipline_raw).strip().lower() if pred_is_discipline_raw is not None else "false"
        gold_is_discipline_str = str(gold_is_discipline_raw).strip().lower() if gold_is_discipline_raw is not None else "false"
        
        pred_is_discipline = pred_is_discipline_str in ["true", "yes", "1"]
        gold_is_discipline = gold_is_discipline_str in ["true", "yes", "1"]

        correct = pred_is_discipline == gold_is_discipline

        score = 1.0 if correct else 0.0

        if hasattr(pred, "reasoning") and pred.reasoning:
            reasoning_len = len(pred.reasoning.strip())
            if reasoning_len < 20:
                score *= 0.8
                feedback = "Reasoning too brief. Provide more detailed explanation."
            elif reasoning_len > 500:
                score *= 0.9
                feedback = "Reasoning too verbose. Be more concise."
            else:
                feedback = "Good classification with appropriate reasoning."
        else:
            score *= 0.5
            feedback = "Missing reasoning. Always explain your decision."

        if not correct:
            # Provide specific feedback based on the term and expected answer
            term = gold.term.lower()

            if pred_is_discipline and not gold_is_discipline:
                # False positive - should have said False
                if term in [
                    "surgery",
                    "pediatrics",
                    "neurology",
                    "radiology",
                    "ophthalmology",
                    "gynecology",
                    "obstetrics",
                ]:
                    feedback = f"'{term}' is a medical specialty within medicine, not a top-level discipline."
                elif term in [
                    "microbiology",
                    "immunology",
                    "neurobiology",
                    "dermatology",
                ]:
                    feedback = f"'{term}' is a sub-field of biology/medicine, not a standalone academic discipline."
                elif term in ["literatures", "computing"]:
                    feedback = f"'{term}' is too vague - the actual disciplines would be 'literature' or 'computer science'."
                elif term == "race":
                    feedback = f"'{term}' is a social descriptor, not an academic discipline. Related field would be 'ethnic studies'."
                else:
                    feedback = f"'{term}' is too specific/specialized. Ask: would this have its own college at most universities?"
            else:
                # False negative - should have said True
                if term in [
                    "engineering",
                    "medicine",
                    "business",
                    "law",
                    "humanities",
                    "mathematics",
                    "physics",
                ]:
                    feedback = f"'{term}' is a fundamental academic discipline with its own colleges at most universities."
                elif "science" in term or term in [
                    "psychology",
                    "sociology",
                    "anthropology",
                ]:
                    feedback = f"'{term}' is a recognized academic field taught as a major discipline."
                else:
                    feedback = f"'{term}' is a legitimate academic discipline - check if it would have its own college/school."

        return dspy.Prediction(score=score, feedback=feedback)

    except Exception as e:
        return dspy.Prediction(
            score=0.0,
            feedback=f"Error evaluating prediction: {str(e)}. Ensure proper output format.",
        )


# Default prompts - aligned with actual implementation in lv0_s3_verify_single_token.py
DEFAULT_S3_SYSTEM = """You are an expert in academic research classification with a deep understanding of research domains, 
academic departments, scientific disciplines, and specialized fields of study.

Your task is to verify whether terms represent legitimate broad academic disciplines by considering:
1. Academic relevance - Is it a recognized field of study or broad academic discipline?
2. Disciplinary context - Does it represent a major division of knowledge in academia?
3. Scope - Is it broad enough to encompass multiple research areas or subdisciplines?

Accept:
- Broad academic disciplines (e.g., humanities, sciences, engineering)
- Major fields of study (e.g., arts, medicine, law)
- Traditional knowledge domains (e.g., social sciences, natural sciences)

DO NOT accept:
- Narrow specializations or subdisciplines (e.g., organic chemistry, medieval history)
- Technical methodologies (e.g., spectroscopy, chromatography)
- Specific research topics (e.g., climate change, artificial intelligence)
- Acronyms (e.g., STEM, AI) unless they are universally recognized as standalone concepts
- Proper nouns or names (e.g., Harvard, MIT)
- Informal or colloquial terms (e.g., stuff, thing)
- General English words without specific academic meaning"""

DEFAULT_S3_USER = """Analyze whether "{keyword}" is a valid broad academic discipline.

Evidence - Colleges/schools/divisions that mention this concept:
{evidence_colleges}

Consider:
1. Is it a recognized major field of study or broad academic discipline?
2. Does it represent a major division of knowledge in academia?
3. Is it broad enough to encompass multiple research areas or subdisciplines?

Answer with true if it meets these criteria, false otherwise."""


def optimize_prompts():
    """
    Run GEPA optimization for discipline verification prompts with signature metadata extraction.
    
    This function uses the enhanced run_optimization() which automatically:
    1. Extracts signature metadata from the GEPA optimization results
    2. Saves prompts with DSPy 2024-2025 compliant signature metadata
    3. Enables declarative programming for runtime integration
    4. Supports ChainOfThought predictor type recognition
    5. Provides metric-driven development with reasoning evaluation
    
    The AcademicDisciplineVerification signature will be analyzed to extract:
    - Input fields: term (description: "Term to verify as academic discipline")
    - Output fields: is_discipline, reasoning (enabling comprehensive evaluation)
    - Instructions: Combined system + user prompt instructions
    - Predictor type: ChainOfThought (detected from DisciplineVerifier.prog)
    """
    return run_optimization(
        program_name="lv0_s3_discipline_verification",
        training_data_path=TRAINING_DATA_PATH,
        dspy_module=DisciplineVerifier,
        metric_func=metric_with_feedback,
        prepare_examples_func=prepare_dspy_examples,
        default_system_prompt=DEFAULT_S3_SYSTEM,
        default_user_prompt=DEFAULT_S3_USER,
        create_training_data_func=create_training_data,
    )


if __name__ == "__main__":
    """
    Enhanced lv0_s3 discipline verification optimization with DSPy 2024-2025 features.
    
    This will generate optimized prompts with signature metadata at:
    - data/prompts/lv0_s3_system_latest.json (with signature_metadata field)
    - data/prompts/lv0_s3_user_latest.json (with signature_metadata field)
    
    The signature metadata enables:
    - Declarative DSPy programming at runtime
    - Automatic predictor type selection (ChainOfThought)  
    - Multi-field reasoning evaluation (is_discipline + reasoning)
    - Enhanced modular composition for verification tasks
    
    To validate DSPy compliance after optimization:
    > from generate_glossary.llm.signatures import validate_dspy_compliance
    > result = validate_dspy_compliance("lv0_s3")
    > print(f"DSPy compliance score: {result['overall_score']:.2f}")
    """
    try:
        print("ğŸš€ Starting enhanced lv0_s3 optimization with signature metadata extraction...")
        optimize_prompts()
        print("âœ… Optimization complete! Prompts saved with DSPy 2024-2025 signature metadata.")
        print("ğŸ“‹ Generated files include declarative programming metadata for runtime integration.")
        print("ğŸ”¬ Multi-field reasoning support enables comprehensive verification metrics.")
    except Exception as e:
        print(f"âŒ Error during optimization: {e}")
        sys.exit(1)
